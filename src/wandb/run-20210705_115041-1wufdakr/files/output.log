2021-07-05 11:50:42,847 [INFO] {'data_dir': '../data', 'no_cuda': False, 'debug': True, 'log_batch_size': 7, 'epochs': 10, 'data_type': 'matres', 'finetune': False, 'model': 'box', 'downsample': 0.001, 'learning_rate': 0.01, 'lambda_anno': 1.0, 'lambda_trans': 0.0, 'lambda_cross': 0.0, 'volume_temp': 1.0, 'intersection_temp': 0.0001, 'hieve_threshold': -0.6, 'matres_threshold': -0.5, 'mlp_size': 32, 'mlp_output_dim': 32, 'hieve_mlp_size': 64, 'matres_mlp_size': 32, 'proj_output_dim': 32, 'num_layers': 1, 'roberta_hidden_size': 1024, 'lstm_hidden_size': 256, 'lstm_input_size': 768, 'no_valid': False, 'loss_type': 0, 'patience': 8, 'eval_step': 1, 'eval_type': 'one', 'load_model': 0, 'saved_model': '/Users/ehwang/PycharmProjects/CE2ERE/src/model/matres_20210505163300_2ya6wb7v.pt', 'wandb_id': 'hwang7520/CE2ERE-src/2ya6wb7v', 'load_valid': 0, 'save_plot': 1, 'symm_eval': 1}
2021-07-05 11:50:42,847 [INFO] Requested CUDA but it is not available, running on CPU
  0%|          | 0/274 [00:00<?, ?it/s]  0%|          | 1/274 [00:00<01:00,  4.55it/s]  1%|          | 3/274 [00:00<00:54,  4.95it/s]  2%|▏         | 5/274 [00:00<00:43,  6.16it/s]  3%|▎         | 7/274 [00:00<00:42,  6.26it/s]  3%|▎         | 9/274 [00:01<00:37,  6.99it/s]  3%|▎         | 9/274 [00:01<00:45,  5.80it/s]
Traceback (most recent call last):
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/__main__.py", line 261, in <module>
    main()
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/__main__.py", line 256, in main
    trainer, evaluator = setup(args)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/__main__.py", line 141, in setup
    train_dataloader, valid_dataloader_dict, test_dataloader_dict, valid_cv_dataloader_dict, test_cv_dataloader_dict, num_classes = create_dataloader(args)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/__main__.py", line 42, in create_dataloader
    matres_train_set, matres_valid_set, matres_test_set, matres_valid_cv_set, matres_test_cv_set = matres_data_loader(args, data_dir)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/data_loader.py", line 351, in matres_data_loader
    data_dict = matres_file_reader(dir_path, file_name, eiid_to_event_trigger)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/data_reader.py", line 395, in matres_file_reader
    data_dict = document_to_sentences(data_dict) # sentence information update
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/data_reader.py", line 172, in document_to_sentences
    roberta_subword_to_ID, roberta_subwords, roberta_subword_span, roberta_subword_map = RoBERTa_tokenize(sntc_dict)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/data_reader.py", line 124, in RoBERTa_tokenize
    return RoBERTa_list(sntc_dict["content"], sntc_dict["token_span_SENT"])
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/data_reader.py", line 95, in RoBERTa_list
    encoded = tokenizer.encode(content) # list of integers
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 907, in encode
    **kwargs,
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 1015, in encode_plus
    first_ids = get_input_ids(text)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 995, in get_input_ids
    tokens = self.tokenize(text, add_special_tokens=add_special_tokens, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 812, in tokenize
    tokenized_text = split_on_tokens(added_tokens, text)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 806, in split_on_tokens
    for token in tokenized_text
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_utils.py", line 806, in <genexpr>
    for token in tokenized_text
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_gpt2.py", line 204, in _tokenize
    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(" "))
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/tokenization_gpt2.py", line 158, in bpe
    word = tuple(token)
KeyboardInterrupt
