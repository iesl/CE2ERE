2021-06-27 03:08:07,127 [INFO] {'data_dir': '../data', 'no_cuda': False, 'debug': True, 'log_batch_size': 7, 'epochs': 10, 'data_type': 'hieve', 'finetune': False, 'model': 'box', 'downsample': 0.4, 'learning_rate': 0.01, 'lambda_anno': 1.0, 'lambda_trans': 0.0, 'lambda_cross': 0.0, 'volume_temp': 1.0, 'intersection_temp': 0.0001, 'hieve_threshold': -0.5902, 'matres_threshold': -0.5, 'mlp_size': 32, 'mlp_output_dim': 32, 'hieve_mlp_size': 64, 'matres_mlp_size': 32, 'proj_output_dim': 32, 'num_layers': 1, 'roberta_hidden_size': 1024, 'lstm_hidden_size': 256, 'lstm_input_size': 768, 'seed': 10, 'no_valid': False, 'loss_type': 0, 'patience': 8, 'eval_step': 1, 'eval_type': 'one', 'load_model': 0, 'saved_model': '/Users/ehwang/PycharmProjects/CE2ERE/src/model/matres_20210505163300_2ya6wb7v.pt', 'wandb_id': 'hwang7520/CE2ERE-src/2ya6wb7v', 'load_valid': 0}
2021-06-27 03:08:07,127 [INFO] Requested CUDA but it is not available, running on CPU
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:23,  4.10it/s]  3%|▎         | 3/100 [00:01<00:31,  3.05it/s]  4%|▍         | 4/100 [00:01<00:30,  3.14it/s]  5%|▌         | 5/100 [00:01<00:25,  3.66it/s]  6%|▌         | 6/100 [00:01<00:22,  4.17it/s]  7%|▋         | 7/100 [00:02<00:39,  2.35it/s]  9%|▉         | 9/100 [00:02<00:28,  3.15it/s] 10%|█         | 10/100 [00:02<00:23,  3.79it/s] 11%|█         | 11/100 [00:02<00:19,  4.60it/s] 12%|█▏        | 12/100 [00:03<00:17,  5.05it/s] 14%|█▍        | 14/100 [00:03<00:13,  6.19it/s] 15%|█▌        | 15/100 [00:03<00:12,  6.85it/s] 16%|█▌        | 16/100 [00:03<00:11,  7.13it/s] 17%|█▋        | 17/100 [00:03<00:18,  4.53it/s] 20%|██        | 20/100 [00:04<00:13,  5.74it/s] 22%|██▏       | 22/100 [00:04<00:12,  6.27it/s] 23%|██▎       | 23/100 [00:04<00:12,  6.09it/s] 24%|██▍       | 24/100 [00:04<00:20,  3.68it/s] 26%|██▌       | 26/100 [00:05<00:15,  4.76it/s] 27%|██▋       | 27/100 [00:05<00:23,  3.06it/s] 28%|██▊       | 28/100 [00:05<00:18,  3.80it/s] 29%|██▉       | 29/100 [00:06<00:26,  2.66it/s] 30%|███       | 30/100 [00:06<00:21,  3.21it/s] 33%|███▎      | 33/100 [00:06<00:15,  4.24it/s] 35%|███▌      | 35/100 [00:09<00:39,  1.66it/s] 36%|███▌      | 36/100 [00:09<00:29,  2.20it/s] 37%|███▋      | 37/100 [00:10<00:24,  2.61it/s] 38%|███▊      | 38/100 [00:10<00:20,  3.07it/s] 39%|███▉      | 39/100 [00:13<01:17,  1.27s/it] 41%|████      | 41/100 [00:13<00:53,  1.10it/s] 42%|████▏     | 42/100 [00:14<00:41,  1.41it/s] 43%|████▎     | 43/100 [00:14<00:40,  1.40it/s] 44%|████▍     | 44/100 [00:15<00:31,  1.77it/s] 45%|████▌     | 45/100 [00:15<00:24,  2.24it/s] 47%|████▋     | 47/100 [00:15<00:18,  2.84it/s] 48%|████▊     | 48/100 [00:16<00:23,  2.19it/s] 51%|█████     | 51/100 [00:16<00:16,  2.94it/s] 52%|█████▏    | 52/100 [00:16<00:14,  3.38it/s] 54%|█████▍    | 54/100 [00:17<00:14,  3.12it/s] 56%|█████▌    | 56/100 [00:17<00:11,  3.95it/s] 58%|█████▊    | 58/100 [00:17<00:08,  5.07it/s] 59%|█████▉    | 59/100 [00:17<00:09,  4.40it/s] 60%|██████    | 60/100 [00:18<00:09,  4.24it/s] 62%|██████▏   | 62/100 [00:18<00:07,  5.16it/s] 63%|██████▎   | 63/100 [00:18<00:06,  5.93it/s] 64%|██████▍   | 64/100 [00:18<00:06,  5.27it/s] 66%|██████▌   | 66/100 [00:18<00:05,  6.24it/s] 69%|██████▉   | 69/100 [00:19<00:04,  6.80it/s] 71%|███████   | 71/100 [00:19<00:03,  8.29it/s] 73%|███████▎  | 73/100 [00:19<00:03,  8.22it/s] 75%|███████▌  | 75/100 [00:19<00:03,  8.33it/s] 77%|███████▋  | 77/100 [00:20<00:02,  8.52it/s] 78%|███████▊  | 78/100 [00:20<00:02,  8.73it/s] 79%|███████▉  | 79/100 [00:20<00:03,  6.27it/s] 80%|████████  | 80/100 [00:20<00:02,  6.74it/s] 81%|████████  | 81/100 [00:20<00:03,  5.99it/s] 83%|████████▎ | 83/100 [00:20<00:02,  7.44it/s] 85%|████████▌ | 85/100 [00:21<00:01,  8.13it/s] 87%|████████▋ | 87/100 [00:21<00:01,  8.54it/s] 89%|████████▉ | 89/100 [00:21<00:01,  8.72it/s] 91%|█████████ | 91/100 [00:21<00:00,  9.54it/s] 93%|█████████▎| 93/100 [00:22<00:00,  7.97it/s] 94%|█████████▍| 94/100 [00:22<00:00,  8.13it/s] 95%|█████████▌| 95/100 [00:22<00:00,  6.56it/s] 97%|█████████▋| 97/100 [00:22<00:00,  8.13it/s] 99%|█████████▉| 99/100 [00:22<00:00,  8.13it/s]100%|██████████| 100/100 [00:22<00:00,  4.39it/s]
2021-06-27 03:08:34,138 [INFO] HiEve Preprocessing took 0:00:27
2021-06-27 03:08:34,138 [INFO] HiEve training instance num: 31090, valid instance num: 7107, test instance num: 6314
2021-06-27 03:08:34,138 [INFO] debug mode on
2021-06-27 03:08:34,699 [DEBUG] Starting new HTTPS connection (1): s3.amazonaws.com:443
2021-06-27 03:08:35,565 [DEBUG] https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/roberta-base-config.json HTTP/1.1" 200 0
2021-06-27 03:08:35,571 [INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /Users/ehwang/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690
2021-06-27 03:08:35,572 [INFO] Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_id": 2,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-05,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 1,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 1,
  "use_bfloat16": false,
  "vocab_size": 50265
}

2021-06-27 03:08:35,577 [DEBUG] Starting new HTTPS connection (1): s3.amazonaws.com:443
2021-06-27 03:08:36,418 [DEBUG] https://s3.amazonaws.com:443 "HEAD /models.huggingface.co/bert/roberta-base-pytorch_model.bin HTTP/1.1" 200 0
2021-06-27 03:08:36,423 [INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /Users/ehwang/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
Using OneThresholdEvaluator..!
2021-06-27 03:08:39,335 [INFO] ======== Epoch 1 / 10 ========
2021-06-27 03:08:39,335 [INFO] Training start...
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:39<00:00, 39.98s/it]100%|██████████| 1/1 [00:39<00:00, 39.98s/it]
2021-06-27 03:09:19,315 [INFO] epoch: 1, loss: 266.798584
2021-06-27 03:09:19,316 [INFO] Validation-[valid-hieve] start... 
2021-06-27 03:09:57,385 [INFO] confusion_matrix: 
[[22  0  0  0]
 [12  0  0  7]
 [35  0  7  0]
 [17  0  0  0]]
/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
2021-06-27 03:09:57,395 [INFO] classifiction_report: 
              precision    recall  f1-score   support

          00       0.26      1.00      0.41        22
          01       0.00      0.00      0.00        19
          10       1.00      0.17      0.29        42
          11       0.00      0.00      0.00        17

    accuracy                           0.29       100
   macro avg       0.31      0.29      0.17       100
weighted avg       0.48      0.29      0.21       100

2021-06-27 03:09:57,395 [INFO] macro f1 score: 0.1429
2021-06-27 03:09:57,395 [INFO] done!
1002021-06-27 03:09:57,400 [INFO] Validation-[cv-valid-hieve] start... 
 100 100 100 100
Traceback (most recent call last):
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/__main__.py", line 252, in <module>
    main()
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/__main__.py", line 248, in main
    trainer.train()
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/train.py", line 154, in train
    self.evaluation(epoch)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/train.py", line 170, in evaluation
    cv_valid_metrics.update(self.evaluator.evaluate("hieve", "cv-valid"))
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/train.py", line 246, in evaluate
    vol_A_B, vol_B_A, vol_B_C, vol_C_B, vol_A_C, vol_C_A = self.model(batch, device, self.train_type) # [batch_size, 2]
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/model.py", line 385, in forward
    roberta_x_sntc = self._get_roberta_embedding(x_sntc) #[64, 120, 768];[batch_size, padded_len, roberta_dim]
  File "/Users/ehwang/PycharmProjects/CE2ERE/src/model.py", line 375, in _get_roberta_embedding
    roberta_embd = self.RoBERTa_layer(s.unsqueeze(0))[0] # [1, 120, 768]
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 790, in forward
    encoder_attention_mask=encoder_extended_attention_mask,
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 407, in forward
    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 368, in forward
    self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 314, in forward
    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/transformers/modeling_bert.py", line 216, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/Users/ehwang/opt/anaconda3/envs/conda-env/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
